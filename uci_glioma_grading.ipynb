{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPTP7nkr7wl7oBIVJB9hVkC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhp7wmurH6fN"
      },
      "outputs": [],
      "source": [
        "# This notebook will guide you through the process of loading, analyzing, and utilizing\n",
        "# a glioma dataset from the UCI Machine Learning Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "id": "610yRs_zIYeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have the necessary library installed, let's fetch the glioma dataset from the UCI repository.\n",
        "# This dataset contains clinical and molecular features used for glioma grading.\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# Fetch dataset\n",
        "glioma_grading_clinical_and_mutation_features = fetch_ucirepo(id=759)\n",
        "\n",
        "# Data (as pandas dataframes)\n",
        "X = glioma_grading_clinical_and_mutation_features.data.features\n",
        "y = glioma_grading_clinical_and_mutation_features.data.targets\n",
        "\n",
        "# Metadata\n",
        "print(glioma_grading_clinical_and_mutation_features.metadata)\n",
        "\n",
        "# Variable information\n",
        "print(glioma_grading_clinical_and_mutation_features.variables)\n"
      ],
      "metadata": {
        "id": "1QV3Puq-IubY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have the dataset, let's explore it to understand its structure and content.\n",
        "# We will start by examining the first few rows of the features and target dataframes.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Display the first few rows of the features dataframe\n",
        "print(\"Features:\")\n",
        "print(X.head())\n",
        "\n",
        "# Display the first few rows of the target dataframe\n",
        "print(\"\\nTargets:\")\n",
        "print(y.head())\n",
        "\n",
        "# Check for missing values in the dataset\n",
        "print(\"\\nMissing values in features:\")\n",
        "print(X.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing values in targets:\")\n",
        "print(y.isnull().sum())\n",
        "\n",
        "# Replace missing values in numeric columns with the mean of the column\n",
        "for column in X.select_dtypes(include=['number']).columns:\n",
        "    X[column].fillna(X[column].mean(), inplace=True)\n",
        "\n",
        "# Replace missing values in non-numeric columns with the mode of the column\n",
        "for column in X.select_dtypes(exclude=['number']).columns:\n",
        "    X[column].fillna(X[column].mode()[0], inplace=True)\n",
        "\n",
        "# Handle missing values in the target variable 'y' if it's numeric\n",
        "if y.dtypes['Grade'] in ['int64', 'float64']:\n",
        "    y.fillna(y.mean(), inplace=True)\n"
      ],
      "metadata": {
        "id": "3a3-0aqTK43Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's split the dataset into training and testing sets for model training and evaluation.\n",
        "# We will use 80% of the data for training and 20% for testing.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shape of the training and testing sets\n",
        "print(\"Training set shape (features):\", X_train.shape)\n",
        "print(\"Training set shape (targets):\", y_train.shape)\n",
        "print(\"Testing set shape (features):\", X_test.shape)\n",
        "print(\"Testing set shape (targets):\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "Fuf50Ui-M9-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define a simple neural network architecture using PyTorch.\n",
        "# We will also ensure that the model uses GPU resources if available.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define a simple neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 50\n",
        "output_size = 1\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size, output_size).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "u4nMIVRxNNMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's encode the categorical variables before converting the pandas dataframes into PyTorch tensors.\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode categorical columns\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X_train[col] = le.fit_transform(X_train[col])\n",
        "    X_test[col] = le.transform(X_test[col])\n",
        "\n",
        "# Convert the pandas dataframes into PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "# Create DataLoader objects for training and testing datasets\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Verify the shapes of the data\n",
        "print(\"Training set tensor shape:\", X_train_tensor.shape, y_train_tensor.shape)\n",
        "print(\"Testing set tensor shape:\", X_test_tensor.shape, y_test_tensor.shape)\n",
        "\n",
        "# Verify if data is on GPU\n",
        "print(f\"X_train_tensor is on device: {X_train_tensor.device}\")\n",
        "print(f\"y_train_tensor is on device: {y_train_tensor.device}\")\n",
        "print(f\"X_test_tensor is on device: {X_test_tensor.device}\")\n",
        "print(f\"y_test_tensor is on device: {y_test_tensor.device}\")\n",
        "\n",
        "# Verify if model is on GPU\n",
        "print(f\"Model is on device: {next(model.parameters()).device}\")\n"
      ],
      "metadata": {
        "id": "m2lUnQiRO6p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training loop to train the model using the training dataset\n",
        "\n",
        "num_epochs = 50  # Number of epochs to train the model\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            # Move inputs and labels to GPU if available\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            running_loss += loss.item()\n",
        "            if (i + 1) % 10 == 0:    # Print every 10 batches\n",
        "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer, num_epochs)\n"
      ],
      "metadata": {
        "id": "eb2v28P-PJoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in test_loader:\n",
        "            # Move inputs and labels to GPU if available\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Accuracy of the model on the test set: {accuracy:.2f}%')\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "id": "JqDgwbH4Pbj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced training loop with early stopping functionality\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.best_loss = float('inf')\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decreases.'''\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.best_loss = val_loss\n",
        "\n",
        "# Function to train the model with early stopping\n",
        "def train_model_with_early_stopping(model, train_loader, test_loader, criterion, optimizer, num_epochs, patience):\n",
        "    early_stopping = EarlyStopping(patience=patience)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            # Move inputs and labels to GPU if available\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                # Move inputs and labels to GPU if available\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(test_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        early_stopping(avg_val_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered. Stopping training.\")\n",
        "            model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "            break\n",
        "\n",
        "# Train the model with early stopping\n",
        "train_model_with_early_stopping(model, train_loader, test_loader, criterion, optimizer, num_epochs=50, patience=5)\n"
      ],
      "metadata": {
        "id": "kJmphPgLPtze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the best model from the checkpoint\n",
        "model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "# Evaluate the model with the best state\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "id": "zcpT0GZOQaNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot the training and validation loss\n",
        "def plot_loss(train_losses, val_losses):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss over Epochs')\n",
        "    plt.show()\n",
        "\n",
        "# Function to print model insights and performance details\n",
        "def print_model_insights(model, train_loader, test_loader, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Calculate final training loss\n",
        "        train_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item()\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Calculate final validation loss\n",
        "        val_loss = 0.0\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "        avg_val_loss = val_loss / len(test_loader)\n",
        "\n",
        "        # Calculate final accuracy\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "    print(f\"Final Training Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"Final Validation Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Accuracy on Test Set: {accuracy:.2f}%\")\n",
        "\n",
        "# Store training and validation loss for plotting\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Modified training function to track losses\n",
        "def train_model_with_early_stopping_and_tracking(model, train_loader, test_loader, criterion, optimizer, num_epochs, patience):\n",
        "    early_stopping = EarlyStopping(patience=patience)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            # Move inputs and labels to GPU if available\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                # Move inputs and labels to GPU if available\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(test_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        early_stopping(avg_val_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered. Stopping training.\")\n",
        "            model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "            break\n",
        "\n",
        "# Train the model with early stopping and tracking\n",
        "train_model_with_early_stopping_and_tracking(model, train_loader, test_loader, criterion, optimizer, num_epochs=50, patience=5)\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plot_loss(train_losses, val_losses)\n",
        "\n",
        "# Print the model insights and performance details\n",
        "print_model_insights(model, train_loader, test_loader, criterion)\n"
      ],
      "metadata": {
        "id": "a6pVMfgCRPX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate and plot feature importance\n",
        "def plot_feature_importance(model, feature_names):\n",
        "    # Extract the weights from the first fully connected layer\n",
        "    fc1_weights = model.fc1.weight.cpu().detach().numpy()\n",
        "\n",
        "    # Calculate the absolute sum of weights for each feature\n",
        "    feature_importance = np.sum(np.abs(fc1_weights), axis=0)\n",
        "\n",
        "    # Normalize the importance values\n",
        "    feature_importance = feature_importance / np.sum(feature_importance)\n",
        "\n",
        "    # Create a DataFrame for better visualization\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importance\n",
        "    })\n",
        "\n",
        "    # Sort the DataFrame by importance\n",
        "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Plot the feature importance\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(importance_df['Feature'], importance_df['Importance'])\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Importance')\n",
        "    plt.title('Feature Importance')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the feature importance\n",
        "feature_names = X.columns\n",
        "plot_feature_importance(model, feature_names)\n"
      ],
      "metadata": {
        "id": "8qG1nYCORXFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZvszbhSsR1_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}